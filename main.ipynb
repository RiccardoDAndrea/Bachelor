{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeitserie erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_eries(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # Welle 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # Welle 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # Rauschen\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_eries(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input=(50, 1)),\n",
    "#     keras.layers.Dense(1)\n",
    "# ])\n",
    "\n",
    "# model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "# model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das einfachste RNN das man bauen k√∂nnte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mehrere Zeitschritte vorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_eries(1, n_steps + 10)\n",
    "X_new, Y_new = series[:, :n_steps], series[:, n_steps:]\n",
    "X = X_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "\n",
    "y_pred = X[:, n_steps:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersage des DAX Kurses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/raw/^GDAXI.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_data = len(data)     # rows that data has\n",
    "split_ratio = 0.7           # %70 train + %30 validation\n",
    "length_train = round(length_data * split_ratio)  \n",
    "length_validation = length_data - length_train\n",
    "print(\"Data length :\", length_data)\n",
    "print(\"Train data length :\", length_train)\n",
    "print(\"Validation data lenth :\", length_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:length_train].iloc[:,:2] \n",
    "train_data['Date'] = pd.to_datetime(train_data['Date'])  # converting to date time object\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data[length_train:].iloc[:,:2]\n",
    "validation_data['Date'] = pd.to_datetime(validation_data['Date'])  # converting to date time object\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = train_data.Open.values\n",
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 1d array to 2d array\n",
    "# Changing shape from (1692,) to (1692,1)\n",
    "dataset_train = np.reshape(dataset_train, (-1,1))\n",
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "\n",
    "# scaling dataset\n",
    "dataset_train_scaled = scaler.fit_transform(dataset_train)\n",
    "\n",
    "dataset_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (15,6))\n",
    "plt.plot(dataset_train_scaled)\n",
    "plt.xlabel(\"Days as 1st, 2nd, 3rd..\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "time_step = 50\n",
    "\n",
    "for i in range(time_step, length_train):\n",
    "    X_train.append(dataset_train_scaled[i-time_step:i,0])\n",
    "    y_train.append(dataset_train_scaled[i,0])\n",
    "    \n",
    "# convert list to array\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train before reshape :\",X_train.shape)\n",
    "print(\"Shape of y_train before reshape :\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0],1))\n",
    "\n",
    "print(\"Shape of X_train after reshape :\",X_train.shape)\n",
    "print(\"Shape of y_train after reshape :\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Dropout, Input\n",
    "\n",
    "# initializing the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# defining the input shape\n",
    "regressor.add(Input(shape=(X_train.shape[1], 1)))\n",
    "\n",
    "# adding first RNN layer and dropout regularization\n",
    "regressor.add(SimpleRNN(units=50, activation=\"tanh\", return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding second RNN layer and dropout regularization\n",
    "regressor.add(SimpleRNN(units=50, activation=\"tanh\", return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding third RNN layer and dropout regularization\n",
    "regressor.add(SimpleRNN(units=50, activation=\"tanh\", return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding fourth RNN layer and dropout regularization\n",
    "regressor.add(SimpleRNN(units=50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding the output layer\n",
    "regressor.add(Dense(units=1))\n",
    "\n",
    "# compiling RNN\n",
    "regressor.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
    "\n",
    "# fitting the RNN\n",
    "history = regressor.fit(X_train, y_train, epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Loss vs Epochs\n",
    "plt.figure(figsize =(10,7))\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"Simple RNN model, Loss vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Accuracy vs Epochs\n",
    "plt.figure(figsize =(10,5))\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.title(\"Simple RNN model, Accuracy vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_train)  # predictions\n",
    "y_pred = scaler.inverse_transform(y_pred) # scaling back from 0-1 to original\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scaler.inverse_transform(y_train) # scaling back from 0-1 to original\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "plt.figure(figsize = (30,10))\n",
    "plt.plot(y_pred, color = \"b\", label = \"y_pred\" )\n",
    "plt.plot(y_train, color = \"g\", label = \"y_train\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Open price\")\n",
    "plt.title(\"Simple RNN model, Predictions with input X_train vs y_train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = validation_data.Open.values  # getting \"open\" column and converting to array\n",
    "dataset_validation = np.reshape(dataset_validation, (-1,1))  # converting 1D to 2D array\n",
    "scaled_dataset_validation =  scaler.fit_transform(dataset_validation)  # scaling open values to between 0 and 1\n",
    "print(\"Shape of scaled validation dataset :\",scaled_dataset_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X_test and y_test\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(time_step, length_validation):\n",
    "    X_test.append(scaled_dataset_validation[i-time_step:i,0])\n",
    "    y_test.append(scaled_dataset_validation[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to array\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_test before reshape :\",X_test.shape)\n",
    "print(\"Shape of y_test before reshape :\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))  # reshape to 3D array\n",
    "y_test = np.reshape(y_test, (-1,1))  # reshape to 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_test after reshape :\",X_test.shape)\n",
    "print(\"Shape of y_test after reshape :\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions with X_test data\n",
    "y_pred_of_test = regressor.predict(X_test)\n",
    "# scaling back from 0-1 to original\n",
    "y_pred_of_test = scaler.inverse_transform(y_pred_of_test) \n",
    "print(\"Shape of y_pred_of_test :\",y_pred_of_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "plt.figure(figsize = (30,10))\n",
    "plt.plot(y_pred_of_test, label = \"y_pred_of_test\", c = \"orange\")\n",
    "plt.plot(scaler.inverse_transform(y_test), label = \"y_test\", c = \"g\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Open price\")\n",
    "plt.title(\"Simple RNN model, Prediction with input X_test vs y_test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.subplots(figsize =(30,12))\n",
    "plt.plot(train_data.Date, train_data.Open, label = \"train_data\", color = \"b\")\n",
    "plt.plot(validation_data.Date, validation_data.Open, label = \"validation_data\", color = \"g\")\n",
    "plt.plot(train_data.Date.iloc[time_step:], y_pred, label = \"y_pred\", color = \"r\")\n",
    "plt.plot(validation_data.Date.iloc[time_step:], y_pred_of_test, label = \"y_pred_of_test\", color = \"orange\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Open price\")\n",
    "plt.title(\"Simple RNN model, Train-Validation-Prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scaler.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(\n",
    "    LSTM(64,return_sequences=True,input_shape = (X_train.shape[1],1))) #64 lstm neuron block\n",
    "model_lstm.add(\n",
    "    LSTM(64, return_sequences= False))\n",
    "model_lstm.add(Dense(32))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss = \"mean_squared_error\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "history2 = model_lstm.fit(X_train, y_train, epochs = 10, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,5))\n",
    "plt.plot(history2.history[\"loss\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"LSTM model, Accuracy vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize =(30,12))\n",
    "plt.plot(scaler.inverse_transform(model_lstm.predict(X_test)), label = \"y_pred_of_test\", c = \"orange\" )\n",
    "plt.plot(scaler.inverse_transform(y_test), label = \"y_test\", color = \"g\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Open price\")\n",
    "plt.title(\"LSTM model, Predictions with input X_test vs y_test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = data.iloc[-time_step:].Open.values               # getting last 50 rows and converting to array\n",
    "X_input = scaler.fit_transform(X_input.reshape(-1,1))      # converting to 2D array and scaling\n",
    "X_input = np.reshape(X_input, (1,50,1))                    # reshaping : converting to 3D array\n",
    "print(\"Shape of X_input :\", X_input.shape)\n",
    "X_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN_prediction = scaler.inverse_transform(regressor.predict(X_input))\n",
    "LSTM_prediction = scaler.inverse_transform(model_lstm.predict(X_input))\n",
    "print(\"Simple RNN, Open price prediction for 3/18/2017      :\", simple_RNN_prediction[0,0])\n",
    "print(\"LSTM prediction, Open price prediction for 3/18/2017 :\", LSTM_prediction[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/raw/^GDAXI.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "X_var = data[[\"Open\",\"Close\"]]\n",
    "train_slider = 60\n",
    "validation_slider = 20\n",
    "n_steps= 50\n",
    "\n",
    "X_train, y_train = data[[\"Open\",\"Close\"]][:int(len(data)*train_slider/100)], data[\"Date\"][:int(len(data)*train_slider/100)]\n",
    "X_val, y_val = data[[\"Open\",\"Close\"]][:int(len(data)*validation_slider/100)], data[\"Date\"][:int(len(data)*validation_slider/100)]\n",
    "print(data.shape,X_train.shape, y_train.shape)\n",
    "print(data.shape,X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.to_datetime(y_train)\n",
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = X_train.Open\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dataset_train = np.reshape(dataset_train, (-1,1))\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "\n",
    "# scaling dataset\n",
    "dataset_train_scaled = scaler.fit_transform(dataset_train)\n",
    "\n",
    "dataset_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (15,6))\n",
    "plt.plot(dataset_train_scaled)\n",
    "plt.xlabel(\"Days as 1st, 2nd, 3rd..\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/raw/^GDAXI.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "X_var = data[[\"Open\",\"Close\"]]\n",
    "train_slider = 60\n",
    "validation_slider = 20\n",
    "n_steps= 50\n",
    "\n",
    "X_train, y_train = data[[\"Open\",\"Close\"]][:int(len(data)*train_slider/100), :n_steps], data[\"Date\"][:int(len(data)*train_slider/100), -1]\n",
    "X_val, y_val = data[[\"Open\",\"Close\"]][:int(len(data)*validation_slider/100)], data[\"Date\"][:int(len(data)*validation_slider/100)]\n",
    "print(data.shape,X_train.shape, y_train.shape)\n",
    "print(data.shape,X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/bnsreenu/python_for_microscopists/blob/master/166a-Intro_to_time_series_Forecasting_using_LSTM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from keras.callbacks import EarlyStopping\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.13.1-cp38-cp38-macosx_12_0_arm64.whl.metadata (2.6 kB)\n",
      "INFO: pip is looking at multiple versions of tensorflow to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading tensorflow-2.13.0-cp38-cp38-macosx_12_0_arm64.whl.metadata (2.6 kB)\n",
      "Collecting tensorflow-macos==2.13.0 (from tensorflow)\n",
      "  Downloading tensorflow_macos-2.13.0-cp38-cp38-macosx_12_0_arm64.whl.metadata (3.2 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=23.1.21 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached flatbuffers-24.3.25-py2.py3-none-any.whl.metadata (850 bytes)\n",
      "Collecting gast<=0.4.0,>=0.2.1 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading gast-0.4.0-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting h5py>=2.9.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading h5py-3.11.0-cp38-cp38-macosx_11_0_arm64.whl.metadata (2.5 kB)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.24.3)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: packaging in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (23.2)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: setuptools in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from tensorflow-macos==2.13.0->tensorflow) (1.16.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading termcolor-2.4.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Collecting typing-extensions<4.6.0,>=3.6.6 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading typing_extensions-4.5.0-py3-none-any.whl.metadata (8.5 kB)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading wrapt-1.16.0-cp38-cp38-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading grpcio-1.64.1-cp38-cp38-macosx_10_9_universal2.whl.metadata (3.3 kB)\n",
      "Collecting tensorboard<2.14,>=2.13 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting keras<2.14,>=2.13.1 (from tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading keras-2.13.1-py3-none-any.whl.metadata (2.4 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from astunparse>=1.6.0->tensorflow-macos==2.13.0->tensorflow) (0.43.0)\n",
      "Collecting google-auth<3,>=1.6.3 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached google_auth-2.30.0-py2.py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting google-auth-oauthlib<1.1,>=0.5 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl.metadata (2.7 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading Markdown-3.6-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.31.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Downloading werkzeug-3.0.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (5.3.3)\n",
      "Collecting pyasn1-modules>=0.2.1 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached pyasn1_modules-0.4.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Collecting rsa<5,>=3.1.4 (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached rsa-4.9-py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting requests-oauthlib>=0.7.0 (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (7.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (2.1.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /Users/riccardo/anaconda3/envs/LLM_RAG/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow) (3.17.0)\n",
      "Collecting pyasn1<0.7.0,>=0.4.6 (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached pyasn1-0.6.0-py2.py3-none-any.whl.metadata (8.3 kB)\n",
      "Collecting oauthlib>=3.0.0 (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow-macos==2.13.0->tensorflow)\n",
      "  Using cached oauthlib-3.2.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Downloading tensorflow-2.13.0-cp38-cp38-macosx_12_0_arm64.whl (1.9 kB)\n",
      "Downloading tensorflow_macos-2.13.0-cp38-cp38-macosx_12_0_arm64.whl (189.3 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m189.3/189.3 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m133.7/133.7 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached flatbuffers-24.3.25-py2.py3-none-any.whl (26 kB)\n",
      "Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.64.1-cp38-cp38-macosx_10_9_universal2.whl (10.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading h5py-3.11.0-cp38-cp38-macosx_11_0_arm64.whl (2.9 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading libclang-18.1.1-py2.py3-none-macosx_11_0_arm64.whl (26.4 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m26.4/26.4 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading termcolor-2.4.0-py3-none-any.whl (7.7 kB)\n",
      "Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
      "Downloading wrapt-1.16.0-cp38-cp38-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached google_auth-2.30.0-py2.py3-none-any.whl (193 kB)\n",
      "Using cached google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Downloading Markdown-3.6-py3-none-any.whl (105 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m105.4/105.4 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Downloading werkzeug-3.0.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m227.3/227.3 kB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached pyasn1_modules-0.4.0-py3-none-any.whl (181 kB)\n",
      "Using cached requests_oauthlib-2.0.0-py2.py3-none-any.whl (24 kB)\n",
      "Using cached rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Using cached oauthlib-3.2.2-py3-none-any.whl (151 kB)\n",
      "Using cached pyasn1-0.6.0-py2.py3-none-any.whl (85 kB)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, typing-extensions, termcolor, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, oauthlib, keras, h5py, grpcio, google-pasta, gast, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, markdown, google-auth, google-auth-oauthlib, tensorboard, tensorflow-macos, tensorflow\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.11.0\n",
      "    Uninstalling typing_extensions-4.11.0:\n",
      "      Successfully uninstalled typing_extensions-4.11.0\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.15.0\n",
      "    Uninstalling keras-2.15.0:\n",
      "      Successfully uninstalled keras-2.15.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "sqlalchemy 2.0.30 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic 2.7.1 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "pydantic-core 2.18.2 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
      "torch 2.3.0 requires typing-extensions>=4.8.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed absl-py-2.1.0 astunparse-1.6.3 flatbuffers-24.3.25 gast-0.4.0 google-auth-2.30.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.64.1 h5py-3.11.0 keras-2.13.1 libclang-18.1.1 markdown-3.6 oauthlib-3.2.2 opt-einsum-3.3.0 pyasn1-0.6.0 pyasn1-modules-0.4.0 requests-oauthlib-2.0.0 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.2 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-macos-2.13.0 termcolor-2.4.0 typing-extensions-4.5.0 werkzeug-3.0.3 wrapt-1.16.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"data/raw/^GDAXI.csv\", usecols=[1])\n",
    "dataframe = dataframe.dropna()\n",
    "#plt.plot(dataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9213, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert pandas dataframe to numpy array\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "dataset.shape\n",
    "# Datentypen √§ndern von int64 zu float32\n",
    "# sobald die values sind die bereits float 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9213, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) #Also try QuantileTransformer\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "dataset.shape\n",
    "\n",
    "# Daten umarrangerien damit sie eine Saklierung haben von 0 bis 1. Wobei 1 = max und 0 = min\n",
    "#scaler.fit = transformiert erst die daten die zeile davor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.66)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "\n",
    "# daten aufteilen in train und test\n",
    "# validation data kommt er sp√§ter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sequences(dataset, seq_size=1):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(dataset)-seq_size-1):\n",
    "        #print(i)\n",
    "        window = dataset[i:(i+seq_size), 0]\n",
    "        x.append(window)\n",
    "        y.append(dataset[i+seq_size, 0])\n",
    "        \n",
    "    return np.array(x),np.array(y)\n",
    "\n",
    "\n",
    "# Dieser Prozess hilft dem RNN-Modell, zeitliche Abh√§ngigkeiten \n",
    "# und Muster im Datensatz zu lernen, indem es auf fr√ºhere Werte in \n",
    "# den Sequenzen zur√ºckgreift, um zuk√ºnftige Werte vorherzusagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (6074, 5)\n",
      "Shape of test set: (3127, 5)\n"
     ]
    }
   ],
   "source": [
    "seq_size = 5  # Number of time steps to look back \n",
    "#Larger sequences (look further back) may improve forecasting.\n",
    "\n",
    "trainX, trainY = to_sequences(train, seq_size)\n",
    "testX, testY = to_sequences(test, seq_size)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Shape of training set: {}\".format(trainX.shape))\n",
    "print(\"Shape of test set: {}\".format(testX.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape#[0]\n",
    "#trainX.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Single LSTM with hidden Dense...\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                17920     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 17985 (70.25 KB)\n",
      "Trainable params: 17985 (70.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Train...\n"
     ]
    }
   ],
   "source": [
    "print('Single LSTM with hidden Dense...')\n",
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(None, seq_size)))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "\n",
    "\n",
    "model.summary()\n",
    "print('Train...')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(Sequential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "190/190 - 0s - loss: 2.9738e-05 - val_loss: 2.7363e-04 - 206ms/epoch - 1ms/step\n",
      "Epoch 2/100\n",
      "190/190 - 0s - loss: 2.9590e-05 - val_loss: 3.9530e-04 - 172ms/epoch - 908us/step\n",
      "Epoch 3/100\n",
      "190/190 - 0s - loss: 2.8766e-05 - val_loss: 2.5813e-04 - 185ms/epoch - 975us/step\n",
      "Epoch 4/100\n",
      "190/190 - 0s - loss: 2.8176e-05 - val_loss: 2.3858e-04 - 215ms/epoch - 1ms/step\n",
      "Epoch 5/100\n",
      "190/190 - 0s - loss: 2.7865e-05 - val_loss: 3.6665e-04 - 159ms/epoch - 839us/step\n",
      "Epoch 6/100\n",
      "190/190 - 0s - loss: 2.8193e-05 - val_loss: 3.6064e-04 - 161ms/epoch - 846us/step\n",
      "Epoch 7/100\n",
      "190/190 - 0s - loss: 2.6928e-05 - val_loss: 2.8635e-04 - 159ms/epoch - 837us/step\n",
      "Epoch 8/100\n",
      "190/190 - 0s - loss: 2.6208e-05 - val_loss: 3.0594e-04 - 159ms/epoch - 838us/step\n",
      "Epoch 9/100\n",
      "190/190 - 0s - loss: 2.5463e-05 - val_loss: 2.6566e-04 - 166ms/epoch - 876us/step\n",
      "Epoch 10/100\n",
      "190/190 - 0s - loss: 2.4742e-05 - val_loss: 3.2276e-04 - 196ms/epoch - 1ms/step\n",
      "Epoch 11/100\n",
      "190/190 - 0s - loss: 2.3891e-05 - val_loss: 2.5709e-04 - 162ms/epoch - 850us/step\n",
      "Epoch 12/100\n",
      "190/190 - 0s - loss: 2.4741e-05 - val_loss: 2.7241e-04 - 173ms/epoch - 908us/step\n",
      "Epoch 13/100\n",
      "190/190 - 0s - loss: 2.2780e-05 - val_loss: 3.1325e-04 - 188ms/epoch - 991us/step\n",
      "Epoch 14/100\n",
      "190/190 - 0s - loss: 2.3027e-05 - val_loss: 1.5635e-04 - 178ms/epoch - 937us/step\n",
      "Epoch 15/100\n",
      "190/190 - 0s - loss: 2.2313e-05 - val_loss: 2.0333e-04 - 168ms/epoch - 883us/step\n",
      "Epoch 16/100\n",
      "190/190 - 0s - loss: 2.0985e-05 - val_loss: 2.5644e-04 - 166ms/epoch - 874us/step\n",
      "Epoch 17/100\n",
      "190/190 - 0s - loss: 2.0775e-05 - val_loss: 2.8423e-04 - 240ms/epoch - 1ms/step\n",
      "Epoch 18/100\n",
      "190/190 - 0s - loss: 2.0267e-05 - val_loss: 2.5754e-04 - 165ms/epoch - 869us/step\n",
      "Epoch 19/100\n",
      "190/190 - 0s - loss: 1.9338e-05 - val_loss: 1.8843e-04 - 175ms/epoch - 923us/step\n",
      "Epoch 20/100\n",
      "190/190 - 0s - loss: 1.9133e-05 - val_loss: 1.2849e-04 - 168ms/epoch - 882us/step\n",
      "Epoch 21/100\n",
      "190/190 - 0s - loss: 1.8939e-05 - val_loss: 2.1640e-04 - 170ms/epoch - 895us/step\n",
      "Epoch 22/100\n",
      "190/190 - 0s - loss: 1.8925e-05 - val_loss: 2.6771e-04 - 159ms/epoch - 839us/step\n",
      "Epoch 23/100\n",
      "190/190 - 0s - loss: 1.9737e-05 - val_loss: 1.3865e-04 - 160ms/epoch - 843us/step\n",
      "Epoch 24/100\n",
      "190/190 - 0s - loss: 1.8291e-05 - val_loss: 1.2237e-04 - 160ms/epoch - 842us/step\n",
      "Epoch 25/100\n",
      "190/190 - 0s - loss: 1.8342e-05 - val_loss: 1.6527e-04 - 162ms/epoch - 854us/step\n",
      "Epoch 26/100\n",
      "190/190 - 0s - loss: 1.8431e-05 - val_loss: 3.0032e-04 - 162ms/epoch - 852us/step\n",
      "Epoch 27/100\n",
      "190/190 - 0s - loss: 1.9033e-05 - val_loss: 1.1991e-04 - 163ms/epoch - 858us/step\n",
      "Epoch 28/100\n",
      "190/190 - 0s - loss: 1.7981e-05 - val_loss: 2.5359e-04 - 163ms/epoch - 858us/step\n",
      "Epoch 29/100\n",
      "190/190 - 0s - loss: 1.6955e-05 - val_loss: 3.3820e-04 - 231ms/epoch - 1ms/step\n",
      "Epoch 30/100\n",
      "190/190 - 0s - loss: 1.7902e-05 - val_loss: 1.7209e-04 - 181ms/epoch - 951us/step\n",
      "Epoch 31/100\n",
      "190/190 - 0s - loss: 1.6421e-05 - val_loss: 1.6304e-04 - 175ms/epoch - 922us/step\n",
      "Epoch 32/100\n",
      "190/190 - 0s - loss: 1.6857e-05 - val_loss: 2.2357e-04 - 162ms/epoch - 854us/step\n",
      "Epoch 33/100\n",
      "190/190 - 0s - loss: 1.7882e-05 - val_loss: 1.8289e-04 - 167ms/epoch - 877us/step\n",
      "Epoch 34/100\n",
      "190/190 - 0s - loss: 1.6748e-05 - val_loss: 1.0415e-04 - 160ms/epoch - 840us/step\n",
      "Epoch 35/100\n",
      "190/190 - 0s - loss: 1.7089e-05 - val_loss: 2.5115e-04 - 161ms/epoch - 847us/step\n",
      "Epoch 36/100\n",
      "190/190 - 0s - loss: 1.6786e-05 - val_loss: 1.6459e-04 - 159ms/epoch - 836us/step\n",
      "Epoch 37/100\n",
      "190/190 - 0s - loss: 1.8642e-05 - val_loss: 1.5025e-04 - 161ms/epoch - 848us/step\n",
      "Epoch 38/100\n",
      "190/190 - 0s - loss: 1.6035e-05 - val_loss: 2.6400e-04 - 160ms/epoch - 845us/step\n",
      "Epoch 39/100\n",
      "190/190 - 0s - loss: 1.6286e-05 - val_loss: 2.5772e-04 - 161ms/epoch - 849us/step\n",
      "Epoch 40/100\n",
      "190/190 - 0s - loss: 1.7236e-05 - val_loss: 3.3004e-04 - 160ms/epoch - 841us/step\n",
      "Epoch 41/100\n",
      "190/190 - 0s - loss: 1.5976e-05 - val_loss: 1.8076e-04 - 159ms/epoch - 837us/step\n",
      "Epoch 42/100\n",
      "190/190 - 0s - loss: 1.7083e-05 - val_loss: 1.9552e-04 - 159ms/epoch - 837us/step\n",
      "Epoch 43/100\n",
      "190/190 - 0s - loss: 1.6944e-05 - val_loss: 1.5271e-04 - 160ms/epoch - 843us/step\n",
      "Epoch 44/100\n",
      "190/190 - 0s - loss: 1.5809e-05 - val_loss: 1.2035e-04 - 177ms/epoch - 929us/step\n",
      "Epoch 45/100\n",
      "190/190 - 0s - loss: 1.5960e-05 - val_loss: 1.5925e-04 - 177ms/epoch - 933us/step\n",
      "Epoch 46/100\n",
      "190/190 - 0s - loss: 1.7169e-05 - val_loss: 2.7379e-04 - 161ms/epoch - 845us/step\n",
      "Epoch 47/100\n",
      "190/190 - 0s - loss: 1.5587e-05 - val_loss: 1.5169e-04 - 170ms/epoch - 894us/step\n",
      "Epoch 48/100\n",
      "190/190 - 0s - loss: 1.6493e-05 - val_loss: 2.3430e-04 - 214ms/epoch - 1ms/step\n",
      "Epoch 49/100\n",
      "190/190 - 0s - loss: 1.5702e-05 - val_loss: 1.3957e-04 - 176ms/epoch - 925us/step\n",
      "Epoch 50/100\n",
      "190/190 - 0s - loss: 1.5518e-05 - val_loss: 1.1821e-04 - 171ms/epoch - 901us/step\n",
      "Epoch 51/100\n",
      "190/190 - 0s - loss: 1.5408e-05 - val_loss: 2.8213e-04 - 224ms/epoch - 1ms/step\n",
      "Epoch 52/100\n",
      "190/190 - 0s - loss: 1.7163e-05 - val_loss: 2.2656e-04 - 166ms/epoch - 871us/step\n",
      "Epoch 53/100\n",
      "190/190 - 0s - loss: 1.5742e-05 - val_loss: 1.8784e-04 - 232ms/epoch - 1ms/step\n",
      "Epoch 54/100\n",
      "190/190 - 0s - loss: 1.6002e-05 - val_loss: 2.0937e-04 - 193ms/epoch - 1ms/step\n",
      "Epoch 55/100\n",
      "190/190 - 0s - loss: 1.5961e-05 - val_loss: 1.4912e-04 - 201ms/epoch - 1ms/step\n",
      "Epoch 56/100\n",
      "190/190 - 0s - loss: 1.6450e-05 - val_loss: 1.4665e-04 - 263ms/epoch - 1ms/step\n",
      "Epoch 57/100\n",
      "190/190 - 0s - loss: 1.5099e-05 - val_loss: 1.6965e-04 - 196ms/epoch - 1ms/step\n",
      "Epoch 58/100\n",
      "190/190 - 0s - loss: 1.5676e-05 - val_loss: 2.1491e-04 - 172ms/epoch - 905us/step\n",
      "Epoch 59/100\n",
      "190/190 - 0s - loss: 1.5850e-05 - val_loss: 1.4951e-04 - 181ms/epoch - 953us/step\n",
      "Epoch 60/100\n",
      "190/190 - 0s - loss: 1.6834e-05 - val_loss: 1.5166e-04 - 184ms/epoch - 969us/step\n",
      "Epoch 61/100\n",
      "190/190 - 0s - loss: 1.5702e-05 - val_loss: 3.0972e-04 - 164ms/epoch - 865us/step\n",
      "Epoch 62/100\n",
      "190/190 - 0s - loss: 1.5996e-05 - val_loss: 1.4155e-04 - 160ms/epoch - 844us/step\n",
      "Epoch 63/100\n",
      "190/190 - 0s - loss: 1.6370e-05 - val_loss: 2.7462e-04 - 165ms/epoch - 868us/step\n",
      "Epoch 64/100\n",
      "190/190 - 0s - loss: 1.5523e-05 - val_loss: 1.2900e-04 - 159ms/epoch - 836us/step\n",
      "Epoch 65/100\n",
      "190/190 - 0s - loss: 1.6058e-05 - val_loss: 1.5686e-04 - 186ms/epoch - 977us/step\n",
      "Epoch 66/100\n",
      "190/190 - 0s - loss: 1.5940e-05 - val_loss: 1.9576e-04 - 164ms/epoch - 863us/step\n",
      "Epoch 67/100\n",
      "190/190 - 0s - loss: 1.5625e-05 - val_loss: 1.9430e-04 - 198ms/epoch - 1ms/step\n",
      "Epoch 68/100\n",
      "190/190 - 0s - loss: 1.5881e-05 - val_loss: 1.6205e-04 - 160ms/epoch - 843us/step\n",
      "Epoch 69/100\n",
      "190/190 - 0s - loss: 1.6308e-05 - val_loss: 2.0512e-04 - 156ms/epoch - 822us/step\n",
      "Epoch 70/100\n",
      "190/190 - 0s - loss: 1.6163e-05 - val_loss: 2.2122e-04 - 176ms/epoch - 928us/step\n",
      "Epoch 71/100\n",
      "190/190 - 0s - loss: 1.6388e-05 - val_loss: 1.3449e-04 - 169ms/epoch - 887us/step\n",
      "Epoch 72/100\n",
      "190/190 - 0s - loss: 1.6669e-05 - val_loss: 1.8327e-04 - 165ms/epoch - 869us/step\n",
      "Epoch 73/100\n",
      "190/190 - 0s - loss: 1.5253e-05 - val_loss: 2.3040e-04 - 165ms/epoch - 867us/step\n",
      "Epoch 74/100\n",
      "190/190 - 0s - loss: 1.5288e-05 - val_loss: 1.0494e-04 - 164ms/epoch - 865us/step\n",
      "Epoch 75/100\n",
      "190/190 - 0s - loss: 1.5178e-05 - val_loss: 1.7421e-04 - 165ms/epoch - 867us/step\n",
      "Epoch 76/100\n",
      "190/190 - 0s - loss: 1.5694e-05 - val_loss: 1.3748e-04 - 159ms/epoch - 839us/step\n",
      "Epoch 77/100\n",
      "190/190 - 0s - loss: 1.5472e-05 - val_loss: 2.3801e-04 - 162ms/epoch - 854us/step\n",
      "Epoch 78/100\n",
      "190/190 - 0s - loss: 1.5607e-05 - val_loss: 2.0360e-04 - 160ms/epoch - 842us/step\n",
      "Epoch 79/100\n",
      "190/190 - 0s - loss: 1.5530e-05 - val_loss: 1.6892e-04 - 157ms/epoch - 828us/step\n",
      "Epoch 80/100\n",
      "190/190 - 0s - loss: 1.6290e-05 - val_loss: 1.5605e-04 - 163ms/epoch - 856us/step\n",
      "Epoch 81/100\n",
      "190/190 - 0s - loss: 1.5845e-05 - val_loss: 1.0838e-04 - 160ms/epoch - 843us/step\n",
      "Epoch 82/100\n",
      "190/190 - 0s - loss: 1.5391e-05 - val_loss: 2.5403e-04 - 159ms/epoch - 838us/step\n",
      "Epoch 83/100\n",
      "190/190 - 0s - loss: 1.5103e-05 - val_loss: 1.8041e-04 - 190ms/epoch - 998us/step\n",
      "Epoch 84/100\n",
      "190/190 - 0s - loss: 1.5650e-05 - val_loss: 1.3201e-04 - 159ms/epoch - 835us/step\n",
      "Epoch 85/100\n",
      "190/190 - 0s - loss: 1.5405e-05 - val_loss: 1.2631e-04 - 167ms/epoch - 878us/step\n",
      "Epoch 86/100\n",
      "190/190 - 0s - loss: 1.5510e-05 - val_loss: 1.1134e-04 - 165ms/epoch - 870us/step\n",
      "Epoch 87/100\n",
      "190/190 - 0s - loss: 1.7365e-05 - val_loss: 1.2867e-04 - 159ms/epoch - 834us/step\n",
      "Epoch 88/100\n",
      "190/190 - 0s - loss: 1.6215e-05 - val_loss: 2.2602e-04 - 159ms/epoch - 836us/step\n",
      "Epoch 89/100\n",
      "190/190 - 0s - loss: 1.5356e-05 - val_loss: 9.3938e-05 - 162ms/epoch - 852us/step\n",
      "Epoch 90/100\n",
      "190/190 - 0s - loss: 1.6263e-05 - val_loss: 1.8839e-04 - 165ms/epoch - 869us/step\n",
      "Epoch 91/100\n",
      "190/190 - 0s - loss: 1.5177e-05 - val_loss: 2.2336e-04 - 166ms/epoch - 876us/step\n",
      "Epoch 92/100\n",
      "190/190 - 0s - loss: 1.5705e-05 - val_loss: 2.5666e-04 - 171ms/epoch - 899us/step\n",
      "Epoch 93/100\n",
      "190/190 - 0s - loss: 1.5267e-05 - val_loss: 1.7602e-04 - 161ms/epoch - 847us/step\n",
      "Epoch 94/100\n",
      "190/190 - 0s - loss: 1.6577e-05 - val_loss: 3.2477e-04 - 159ms/epoch - 836us/step\n",
      "Epoch 95/100\n",
      "190/190 - 0s - loss: 1.5466e-05 - val_loss: 1.9044e-04 - 159ms/epoch - 835us/step\n",
      "Epoch 96/100\n",
      "190/190 - 0s - loss: 1.6478e-05 - val_loss: 1.5763e-04 - 159ms/epoch - 838us/step\n",
      "Epoch 97/100\n",
      "190/190 - 0s - loss: 1.4999e-05 - val_loss: 1.8626e-04 - 159ms/epoch - 838us/step\n",
      "Epoch 98/100\n",
      "190/190 - 0s - loss: 1.5383e-05 - val_loss: 2.5100e-04 - 158ms/epoch - 831us/step\n",
      "Epoch 99/100\n",
      "190/190 - 0s - loss: 1.5865e-05 - val_loss: 2.4558e-04 - 159ms/epoch - 836us/step\n",
      "Epoch 100/100\n",
      "190/190 - 0s - loss: 1.6575e-05 - val_loss: 2.3044e-04 - 159ms/epoch - 839us/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x295b29be0>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, validation_data=(testX, testY), verbose=2, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict[:,0].shape, trainPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "#we must shift the predictions so that they align on the x-axis with the original dataset. \n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[seq_size:len(trainPredict)+seq_size, :] = trainPredict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(seq_size*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispiel-Daten\n",
    "data = {'Date': ['2022-06-18 10:30:00', '2022-06-19 11:45:00', '2022-06-20 12:00:00']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sicherstellen, dass die 'Date'-Spalte im DateTime-Format ist\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# # Extrahieren Sie das Datum ohne die Zeitkomponente\n",
    "# df['Date'] = df['Date'].dt.date\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2,offsets1, offsets2 = np.random.rand(4,batch_size, 1)\n",
    "    time = np.linspace(0,1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))    # Welle 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))   # Welle 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)     # + Rauschen\n",
    "    return series[..., np.newaxis].astype(np.float32)\n",
    "n_steps = 50\n",
    "\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "series.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
