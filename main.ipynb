{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.metrics import SparseCategoricalAccuracy\n",
    "import matplotlib.pyplot as plt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zeitserie erstellen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_eries(batch_size, n_steps):\n",
    "    freq1, freq2, offsets1, offsets2 = np.random.rand(4, batch_size, 1)\n",
    "    time = np.linspace(0, 1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))  # Welle 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20)) # Welle 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)   # Rauschen\n",
    "    return series[..., np.newaxis].astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 50\n",
    "series = generate_time_eries(10000, n_steps + 1)\n",
    "X_train, y_train = series[:7000, :n_steps], series[:7000, -1]\n",
    "X_valid, y_valid = series[7000:9000, :n_steps], series[7000:9000, -1]\n",
    "X_test, y_test = series[9000:, :n_steps], series[9000:, -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = keras.models.Sequential([\n",
    "#     keras.layers.Flatten(input=(50, 1)),\n",
    "#     keras.layers.Dense(1)\n",
    "# ])\n",
    "\n",
    "# model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "# model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Das einfachste RNN das man bauen k√∂nnte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.SimpleRNN(1, input_shape=[None, 1])\n",
    "])\n",
    "\n",
    "model.compile(loss=\"mse\", optimizer=\"adam\")\n",
    "model.fit(X_train, y_train, epochs=20, validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mehrere Zeitschritte vorhersagen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "series = generate_time_eries(1, n_steps + 10)\n",
    "X_new, Y_new = series[:, :n_steps], series[:, n_steps:]\n",
    "X = X_new\n",
    "for step_ahead in range(10):\n",
    "    y_pred_one = model.predict(X[:, step_ahead:])[:, np.newaxis, :]\n",
    "    X = np.concatenate([X, y_pred_one], axis=1)\n",
    "\n",
    "y_pred = X[:, n_steps:]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vorhersage des DAX Kurses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/raw/^GDAXI.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.describe())\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()\n",
    "data = data.dropna()\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_data = len(data)     # rows that data has\n",
    "split_ratio = 0.7           # %70 train + %30 validation\n",
    "length_train = round(length_data * split_ratio)  \n",
    "length_validation = length_data - length_train\n",
    "print(\"Data length :\", length_data)\n",
    "print(\"Train data length :\", length_train)\n",
    "print(\"Validation data lenth :\", length_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[:length_train].iloc[:,:2] \n",
    "train_data['Date'] = pd.to_datetime(train_data['Date'])  # converting to date time object\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data = data[length_train:].iloc[:,:2]\n",
    "validation_data['Date'] = pd.to_datetime(validation_data['Date'])  # converting to date time object\n",
    "validation_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = train_data.Open.values\n",
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change 1d array to 2d array\n",
    "# Changing shape from (1692,) to (1692,1)\n",
    "dataset_train = np.reshape(dataset_train, (-1,1))\n",
    "dataset_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "\n",
    "# scaling dataset\n",
    "dataset_train_scaled = scaler.fit_transform(dataset_train)\n",
    "\n",
    "dataset_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (15,6))\n",
    "plt.plot(dataset_train_scaled)\n",
    "plt.xlabel(\"Days as 1st, 2nd, 3rd..\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "y_train = []\n",
    "\n",
    "time_step = 50\n",
    "\n",
    "for i in range(time_step, length_train):\n",
    "    X_train.append(dataset_train_scaled[i-time_step:i,0])\n",
    "    y_train.append(dataset_train_scaled[i,0])\n",
    "    \n",
    "# convert list to array\n",
    "X_train, y_train = np.array(X_train), np.array(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_train before reshape :\",X_train.shape)\n",
    "print(\"Shape of y_train before reshape :\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1],1))\n",
    "y_train = np.reshape(y_train, (y_train.shape[0],1))\n",
    "\n",
    "print(\"Shape of X_train after reshape :\",X_train.shape)\n",
    "print(\"Shape of y_train after reshape :\",y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libraries\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, SimpleRNN, Dropout, Input\n",
    "\n",
    "# initializing the RNN\n",
    "regressor = Sequential()\n",
    "\n",
    "# defining the input shape\n",
    "regressor.add(Input(shape=(X_train.shape[1], 1)))\n",
    "\n",
    "# adding first RNN layer and dropout regularization\n",
    "regressor.add(SimpleRNN(units=50, activation=\"tanh\", return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding second RNN layer and dropout regularization\n",
    "regressor.add(SimpleRNN(units=50, activation=\"tanh\", return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding third RNN layer and dropout regularization\n",
    "regressor.add(SimpleRNN(units=50, activation=\"tanh\", return_sequences=True))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding fourth RNN layer and dropout regularization\n",
    "regressor.add(SimpleRNN(units=50))\n",
    "regressor.add(Dropout(0.2))\n",
    "\n",
    "# adding the output layer\n",
    "regressor.add(Dense(units=1))\n",
    "\n",
    "# compiling RNN\n",
    "regressor.compile(optimizer=\"adam\", loss=\"mean_squared_error\", metrics=[\"accuracy\"])\n",
    "\n",
    "# fitting the RNN\n",
    "history = regressor.fit(X_train, y_train, epochs=50, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history.history[\"loss\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Loss vs Epochs\n",
    "plt.figure(figsize =(10,7))\n",
    "plt.plot(history.history[\"loss\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"Simple RNN model, Loss vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting Accuracy vs Epochs\n",
    "plt.figure(figsize =(10,5))\n",
    "plt.plot(history.history[\"accuracy\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracies\")\n",
    "plt.title(\"Simple RNN model, Accuracy vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = regressor.predict(X_train)  # predictions\n",
    "y_pred = scaler.inverse_transform(y_pred) # scaling back from 0-1 to original\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scaler.inverse_transform(y_train) # scaling back from 0-1 to original\n",
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "plt.figure(figsize = (30,10))\n",
    "plt.plot(y_pred, color = \"b\", label = \"y_pred\" )\n",
    "plt.plot(y_train, color = \"g\", label = \"y_train\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Open price\")\n",
    "plt.title(\"Simple RNN model, Predictions with input X_train vs y_train\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_validation = validation_data.Open.values  # getting \"open\" column and converting to array\n",
    "dataset_validation = np.reshape(dataset_validation, (-1,1))  # converting 1D to 2D array\n",
    "scaled_dataset_validation =  scaler.fit_transform(dataset_validation)  # scaling open values to between 0 and 1\n",
    "print(\"Shape of scaled validation dataset :\",scaled_dataset_validation.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X_test and y_test\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for i in range(time_step, length_validation):\n",
    "    X_test.append(scaled_dataset_validation[i-time_step:i,0])\n",
    "    y_test.append(scaled_dataset_validation[i,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting to array\n",
    "X_test, y_test = np.array(X_test), np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_test before reshape :\",X_test.shape)\n",
    "print(\"Shape of y_test before reshape :\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0],X_test.shape[1],1))  # reshape to 3D array\n",
    "y_test = np.reshape(y_test, (-1,1))  # reshape to 2D array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X_test after reshape :\",X_test.shape)\n",
    "print(\"Shape of y_test after reshape :\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions with X_test data\n",
    "y_pred_of_test = regressor.predict(X_test)\n",
    "# scaling back from 0-1 to original\n",
    "y_pred_of_test = scaler.inverse_transform(y_pred_of_test) \n",
    "print(\"Shape of y_pred_of_test :\",y_pred_of_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualisation\n",
    "plt.figure(figsize = (30,10))\n",
    "plt.plot(y_pred_of_test, label = \"y_pred_of_test\", c = \"orange\")\n",
    "plt.plot(scaler.inverse_transform(y_test), label = \"y_test\", c = \"g\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Open price\")\n",
    "plt.title(\"Simple RNN model, Prediction with input X_test vs y_test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation\n",
    "plt.subplots(figsize =(30,12))\n",
    "plt.plot(train_data.Date, train_data.Open, label = \"train_data\", color = \"b\")\n",
    "plt.plot(validation_data.Date, validation_data.Open, label = \"validation_data\", color = \"g\")\n",
    "plt.plot(train_data.Date.iloc[time_step:], y_pred, label = \"y_pred\", color = \"r\")\n",
    "plt.plot(validation_data.Date.iloc[time_step:], y_pred_of_test, label = \"y_pred_of_test\", color = \"orange\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Open price\")\n",
    "plt.title(\"Simple RNN model, Train-Validation-Prediction\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = scaler.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import LSTM\n",
    "\n",
    "model_lstm = Sequential()\n",
    "model_lstm.add(\n",
    "    LSTM(64,return_sequences=True,input_shape = (X_train.shape[1],1))) #64 lstm neuron block\n",
    "model_lstm.add(\n",
    "    LSTM(64, return_sequences= False))\n",
    "model_lstm.add(Dense(32))\n",
    "model_lstm.add(Dense(1))\n",
    "model_lstm.compile(loss = \"mean_squared_error\", optimizer = \"adam\", metrics = [\"accuracy\"])\n",
    "history2 = model_lstm.fit(X_train, y_train, epochs = 10, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize =(10,5))\n",
    "plt.plot(history2.history[\"loss\"])\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Losses\")\n",
    "plt.title(\"LSTM model, Accuracy vs Epoch\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize =(30,12))\n",
    "plt.plot(scaler.inverse_transform(model_lstm.predict(X_test)), label = \"y_pred_of_test\", c = \"orange\" )\n",
    "plt.plot(scaler.inverse_transform(y_test), label = \"y_test\", color = \"g\")\n",
    "plt.xlabel(\"Days\")\n",
    "plt.ylabel(\"Open price\")\n",
    "plt.title(\"LSTM model, Predictions with input X_test vs y_test\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.iloc[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_input = data.iloc[-time_step:].Open.values               # getting last 50 rows and converting to array\n",
    "X_input = scaler.fit_transform(X_input.reshape(-1,1))      # converting to 2D array and scaling\n",
    "X_input = np.reshape(X_input, (1,50,1))                    # reshaping : converting to 3D array\n",
    "print(\"Shape of X_input :\", X_input.shape)\n",
    "X_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_RNN_prediction = scaler.inverse_transform(regressor.predict(X_input))\n",
    "LSTM_prediction = scaler.inverse_transform(model_lstm.predict(X_input))\n",
    "print(\"Simple RNN, Open price prediction for 3/18/2017      :\", simple_RNN_prediction[0,0])\n",
    "print(\"LSTM prediction, Open price prediction for 3/18/2017 :\", LSTM_prediction[0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/raw/^GDAXI.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "X_var = data[[\"Open\",\"Close\"]]\n",
    "train_slider = 60\n",
    "validation_slider = 20\n",
    "n_steps= 50\n",
    "\n",
    "X_train, y_train = data[[\"Open\",\"Close\"]][:int(len(data)*train_slider/100)], data[\"Date\"][:int(len(data)*train_slider/100)]\n",
    "X_val, y_val = data[[\"Open\",\"Close\"]][:int(len(data)*validation_slider/100)], data[\"Date\"][:int(len(data)*validation_slider/100)]\n",
    "print(data.shape,X_train.shape, y_train.shape)\n",
    "print(data.shape,X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.to_datetime(y_train)\n",
    "y_train.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_train = X_train.Open\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "dataset_train = np.reshape(dataset_train, (-1,1))\n",
    "dataset_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler(feature_range = (0,1))\n",
    "\n",
    "\n",
    "# scaling dataset\n",
    "dataset_train_scaled = scaler.fit_transform(dataset_train)\n",
    "\n",
    "dataset_train_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize = (15,6))\n",
    "plt.plot(dataset_train_scaled)\n",
    "plt.xlabel(\"Days as 1st, 2nd, 3rd..\")\n",
    "plt.ylabel(\"Open Price\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"data/raw/^GDAXI.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(data))\n",
    "X_var = data[[\"Open\",\"Close\"]]\n",
    "train_slider = 60\n",
    "validation_slider = 20\n",
    "n_steps= 50\n",
    "\n",
    "X_train, y_train = data[[\"Open\",\"Close\"]][:int(len(data)*train_slider/100), :n_steps], data[\"Date\"][:int(len(data)*train_slider/100), -1]\n",
    "X_val, y_val = data[[\"Open\",\"Close\"]][:int(len(data)*validation_slider/100)], data[\"Date\"][:int(len(data)*validation_slider/100)]\n",
    "print(data.shape,X_train.shape, y_train.shape)\n",
    "print(data.shape,X_val.shape, y_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# https://github.com/bnsreenu/python_for_microscopists/blob/master/166a-Intro_to_time_series_Forecasting_using_LSTM.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe = pd.read_csv(\"data/raw/^GDAXI.csv\", usecols=[1])\n",
    "dataframe = dataframe.dropna()\n",
    "#plt.plot(dataframe)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9213, 1)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Convert pandas dataframe to numpy array\n",
    "dataset = dataframe.values\n",
    "dataset = dataset.astype('float32')\n",
    "dataset.shape\n",
    "# Datentypen √§ndern von int64 zu float32\n",
    "# sobald die values sind die bereits float 64 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9213, 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LSTM uses sigmoid and tanh that are sensitive to magnitude so values need to be normalized\n",
    "# normalize the dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1)) #Also try QuantileTransformer\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "dataset.shape\n",
    "\n",
    "# Daten umarrangerien damit sie eine Saklierung haben von 0 bis 1. Wobei 1 = max und 0 = min\n",
    "#scaler.fit = transformiert erst die daten die zeile davor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(len(dataset) * 0.66)\n",
    "test_size = len(dataset) - train_size\n",
    "train, test = dataset[0:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "\n",
    "# daten aufteilen in train und test\n",
    "# validation data kommt er sp√§ter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_sequences(dataset, seq_size=1):\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for i in range(len(dataset)-seq_size-1):\n",
    "        #print(i)\n",
    "        window = dataset[i:(i+seq_size), 0]\n",
    "        x.append(window)\n",
    "        y.append(dataset[i+seq_size, 0])\n",
    "        \n",
    "    return np.array(x),np.array(y)\n",
    "\n",
    "\n",
    "# Dieser Prozess hilft dem RNN-Modell, zeitliche Abh√§ngigkeiten \n",
    "# und Muster im Datensatz zu lernen, indem es auf fr√ºhere Werte in \n",
    "# den Sequenzen zur√ºckgreift, um zuk√ºnftige Werte vorherzusagen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training set: (6074, 5)\n",
      "Shape of test set: (3127, 5)\n"
     ]
    }
   ],
   "source": [
    "seq_size = 5  # Number of time steps to look back \n",
    "#Larger sequences (look further back) may improve forecasting.\n",
    "\n",
    "trainX, trainY = to_sequences(train, seq_size)\n",
    "testX, testY = to_sequences(test, seq_size)\n",
    "\n",
    "\n",
    "\n",
    "print(\"Shape of training set: {}\".format(trainX.shape))\n",
    "print(\"Shape of test set: {}\".format(testX.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX.shape#[0]\n",
    "#trainX.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape input to be [samples, time steps, features]\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], 1, trainX.shape[1]))\n",
    "testX = np.reshape(testX, (testX.shape[0], 1, testX.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 64)                17920     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                2080      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 33        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20033 (78.25 KB)\n",
      "Trainable params: 20033 (78.25 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(64, input_shape=(None, seq_size)))\n",
    "model.add(Dense(32))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.13.0\n",
      "Keras version: 2.13.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import tensorflow as tf\n",
    "import keras as keras\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "print(\"Keras version:\", keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "190/190 [==============================] - 1s 2ms/step - loss: 0.0023 - val_loss: 0.0013\n",
      "Epoch 2/100\n",
      "190/190 [==============================] - 0s 947us/step - loss: 3.5463e-05 - val_loss: 9.2273e-04\n",
      "Epoch 3/100\n",
      "190/190 [==============================] - 0s 944us/step - loss: 3.4014e-05 - val_loss: 6.1513e-04\n",
      "Epoch 4/100\n",
      "190/190 [==============================] - 0s 948us/step - loss: 3.2722e-05 - val_loss: 2.0982e-04\n",
      "Epoch 5/100\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 3.0879e-05 - val_loss: 1.2859e-04\n",
      "Epoch 6/100\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 3.0844e-05 - val_loss: 2.4371e-04\n",
      "Epoch 7/100\n",
      "190/190 [==============================] - 0s 969us/step - loss: 3.0293e-05 - val_loss: 1.2771e-04\n",
      "Epoch 8/100\n",
      "190/190 [==============================] - 0s 959us/step - loss: 2.6940e-05 - val_loss: 2.1269e-04\n",
      "Epoch 9/100\n",
      "190/190 [==============================] - 0s 954us/step - loss: 2.8722e-05 - val_loss: 1.7478e-04\n",
      "Epoch 10/100\n",
      "190/190 [==============================] - 0s 945us/step - loss: 2.6692e-05 - val_loss: 1.7483e-04\n",
      "Epoch 11/100\n",
      "190/190 [==============================] - 0s 964us/step - loss: 2.4533e-05 - val_loss: 2.2185e-04\n",
      "Epoch 12/100\n",
      "190/190 [==============================] - 0s 938us/step - loss: 2.4048e-05 - val_loss: 2.5165e-04\n",
      "Epoch 13/100\n",
      "190/190 [==============================] - 0s 939us/step - loss: 2.2774e-05 - val_loss: 2.1538e-04\n",
      "Epoch 14/100\n",
      "190/190 [==============================] - 0s 935us/step - loss: 2.4454e-05 - val_loss: 2.2843e-04\n",
      "Epoch 15/100\n",
      "190/190 [==============================] - 0s 935us/step - loss: 2.2137e-05 - val_loss: 1.4672e-04\n",
      "Epoch 16/100\n",
      "190/190 [==============================] - 0s 930us/step - loss: 2.0488e-05 - val_loss: 2.2585e-04\n",
      "Epoch 17/100\n",
      "190/190 [==============================] - 0s 929us/step - loss: 2.1712e-05 - val_loss: 2.6792e-04\n",
      "Epoch 18/100\n",
      "190/190 [==============================] - 0s 938us/step - loss: 2.0129e-05 - val_loss: 1.8466e-04\n",
      "Epoch 19/100\n",
      "190/190 [==============================] - 0s 934us/step - loss: 2.1379e-05 - val_loss: 2.7834e-04\n",
      "Epoch 20/100\n",
      "190/190 [==============================] - 0s 935us/step - loss: 2.1085e-05 - val_loss: 3.3958e-04\n",
      "Epoch 21/100\n",
      "190/190 [==============================] - 0s 936us/step - loss: 2.0458e-05 - val_loss: 1.1201e-04\n",
      "Epoch 22/100\n",
      "190/190 [==============================] - 0s 935us/step - loss: 1.9832e-05 - val_loss: 1.4015e-04\n",
      "Epoch 23/100\n",
      "190/190 [==============================] - 0s 958us/step - loss: 2.0235e-05 - val_loss: 1.7343e-04\n",
      "Epoch 24/100\n",
      "190/190 [==============================] - 0s 936us/step - loss: 2.0444e-05 - val_loss: 1.8560e-04\n",
      "Epoch 25/100\n",
      "190/190 [==============================] - 0s 945us/step - loss: 1.9935e-05 - val_loss: 1.6708e-04\n",
      "Epoch 26/100\n",
      "190/190 [==============================] - 0s 943us/step - loss: 1.8525e-05 - val_loss: 2.0163e-04\n",
      "Epoch 27/100\n",
      "190/190 [==============================] - 0s 948us/step - loss: 1.9636e-05 - val_loss: 1.6326e-04\n",
      "Epoch 28/100\n",
      "190/190 [==============================] - 0s 937us/step - loss: 1.8451e-05 - val_loss: 2.0822e-04\n",
      "Epoch 29/100\n",
      "190/190 [==============================] - 0s 943us/step - loss: 1.8644e-05 - val_loss: 2.3420e-04\n",
      "Epoch 30/100\n",
      "190/190 [==============================] - 0s 941us/step - loss: 1.8854e-05 - val_loss: 9.4196e-05\n",
      "Epoch 31/100\n",
      "190/190 [==============================] - 0s 971us/step - loss: 1.9289e-05 - val_loss: 1.7599e-04\n",
      "Epoch 32/100\n",
      "190/190 [==============================] - 0s 942us/step - loss: 1.8006e-05 - val_loss: 1.4677e-04\n",
      "Epoch 33/100\n",
      "190/190 [==============================] - 0s 937us/step - loss: 2.1098e-05 - val_loss: 9.0394e-05\n",
      "Epoch 34/100\n",
      "190/190 [==============================] - 0s 973us/step - loss: 1.8267e-05 - val_loss: 1.6105e-04\n",
      "Epoch 35/100\n",
      "190/190 [==============================] - 0s 950us/step - loss: 1.7686e-05 - val_loss: 1.6660e-04\n",
      "Epoch 36/100\n",
      "190/190 [==============================] - 0s 946us/step - loss: 1.9408e-05 - val_loss: 5.2484e-04\n",
      "Epoch 37/100\n",
      "190/190 [==============================] - 0s 943us/step - loss: 1.9117e-05 - val_loss: 1.6553e-04\n",
      "Epoch 38/100\n",
      "190/190 [==============================] - 0s 942us/step - loss: 1.8074e-05 - val_loss: 2.2599e-04\n",
      "Epoch 39/100\n",
      "190/190 [==============================] - 0s 941us/step - loss: 1.9208e-05 - val_loss: 1.3480e-04\n",
      "Epoch 40/100\n",
      "190/190 [==============================] - 0s 947us/step - loss: 1.7632e-05 - val_loss: 8.9902e-05\n",
      "Epoch 41/100\n",
      "190/190 [==============================] - 0s 944us/step - loss: 1.7465e-05 - val_loss: 8.5604e-05\n",
      "Epoch 42/100\n",
      "190/190 [==============================] - 0s 934us/step - loss: 1.8385e-05 - val_loss: 1.0518e-04\n",
      "Epoch 43/100\n",
      "190/190 [==============================] - 0s 943us/step - loss: 1.6758e-05 - val_loss: 2.5015e-04\n",
      "Epoch 44/100\n",
      "190/190 [==============================] - 0s 939us/step - loss: 1.7745e-05 - val_loss: 2.6125e-04\n",
      "Epoch 45/100\n",
      "190/190 [==============================] - 0s 960us/step - loss: 1.7383e-05 - val_loss: 1.9526e-04\n",
      "Epoch 46/100\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 1.8060e-05 - val_loss: 8.3541e-05\n",
      "Epoch 47/100\n",
      "190/190 [==============================] - 0s 968us/step - loss: 1.7192e-05 - val_loss: 1.3272e-04\n",
      "Epoch 48/100\n",
      "190/190 [==============================] - 0s 936us/step - loss: 1.8133e-05 - val_loss: 9.7516e-05\n",
      "Epoch 49/100\n",
      "190/190 [==============================] - 0s 935us/step - loss: 1.8687e-05 - val_loss: 2.4675e-04\n",
      "Epoch 50/100\n",
      "190/190 [==============================] - 0s 945us/step - loss: 1.8653e-05 - val_loss: 3.1173e-04\n",
      "Epoch 51/100\n",
      "190/190 [==============================] - 0s 942us/step - loss: 1.9319e-05 - val_loss: 2.0598e-04\n",
      "Epoch 52/100\n",
      "190/190 [==============================] - 0s 927us/step - loss: 1.6344e-05 - val_loss: 2.2820e-04\n",
      "Epoch 53/100\n",
      "190/190 [==============================] - 0s 929us/step - loss: 1.8466e-05 - val_loss: 1.4653e-04\n",
      "Epoch 54/100\n",
      "190/190 [==============================] - 0s 925us/step - loss: 1.8251e-05 - val_loss: 1.7302e-04\n",
      "Epoch 55/100\n",
      "190/190 [==============================] - 0s 920us/step - loss: 1.7327e-05 - val_loss: 1.0180e-04\n",
      "Epoch 56/100\n",
      "190/190 [==============================] - 0s 948us/step - loss: 1.8590e-05 - val_loss: 2.5489e-04\n",
      "Epoch 57/100\n",
      "190/190 [==============================] - 0s 937us/step - loss: 1.7110e-05 - val_loss: 1.0253e-04\n",
      "Epoch 58/100\n",
      "190/190 [==============================] - 0s 920us/step - loss: 1.8143e-05 - val_loss: 1.8948e-04\n",
      "Epoch 59/100\n",
      "190/190 [==============================] - 0s 921us/step - loss: 1.8035e-05 - val_loss: 1.4971e-04\n",
      "Epoch 60/100\n",
      "190/190 [==============================] - 0s 936us/step - loss: 1.6116e-05 - val_loss: 1.5204e-04\n",
      "Epoch 61/100\n",
      "190/190 [==============================] - 0s 952us/step - loss: 1.8041e-05 - val_loss: 2.7144e-04\n",
      "Epoch 62/100\n",
      "190/190 [==============================] - 0s 923us/step - loss: 1.7580e-05 - val_loss: 1.0868e-04\n",
      "Epoch 63/100\n",
      "190/190 [==============================] - 0s 920us/step - loss: 1.6200e-05 - val_loss: 3.8975e-04\n",
      "Epoch 64/100\n",
      "190/190 [==============================] - 0s 927us/step - loss: 2.0722e-05 - val_loss: 1.2154e-04\n",
      "Epoch 65/100\n",
      "190/190 [==============================] - 0s 919us/step - loss: 1.7901e-05 - val_loss: 2.2983e-04\n",
      "Epoch 66/100\n",
      "190/190 [==============================] - 0s 924us/step - loss: 1.7775e-05 - val_loss: 2.3384e-04\n",
      "Epoch 67/100\n",
      "190/190 [==============================] - 0s 931us/step - loss: 1.5963e-05 - val_loss: 3.8844e-04\n",
      "Epoch 68/100\n",
      "190/190 [==============================] - 0s 949us/step - loss: 1.7647e-05 - val_loss: 1.3891e-04\n",
      "Epoch 69/100\n",
      "190/190 [==============================] - 0s 926us/step - loss: 1.7284e-05 - val_loss: 1.6133e-04\n",
      "Epoch 70/100\n",
      "190/190 [==============================] - 0s 926us/step - loss: 1.7328e-05 - val_loss: 1.0688e-04\n",
      "Epoch 71/100\n",
      "190/190 [==============================] - 0s 920us/step - loss: 1.6494e-05 - val_loss: 1.8723e-04\n",
      "Epoch 72/100\n",
      "190/190 [==============================] - 0s 918us/step - loss: 1.6463e-05 - val_loss: 1.6428e-04\n",
      "Epoch 73/100\n",
      "190/190 [==============================] - 0s 926us/step - loss: 1.8374e-05 - val_loss: 1.9216e-04\n",
      "Epoch 74/100\n",
      "190/190 [==============================] - 0s 958us/step - loss: 1.7698e-05 - val_loss: 3.9719e-04\n",
      "Epoch 75/100\n",
      "190/190 [==============================] - 0s 921us/step - loss: 1.7143e-05 - val_loss: 3.2120e-04\n",
      "Epoch 76/100\n",
      "190/190 [==============================] - 0s 921us/step - loss: 1.6896e-05 - val_loss: 1.6353e-04\n",
      "Epoch 77/100\n",
      "190/190 [==============================] - 0s 929us/step - loss: 1.9742e-05 - val_loss: 8.5942e-05\n",
      "Epoch 78/100\n",
      "190/190 [==============================] - 0s 925us/step - loss: 1.7366e-05 - val_loss: 1.3902e-04\n",
      "Epoch 79/100\n",
      "190/190 [==============================] - 0s 945us/step - loss: 1.7038e-05 - val_loss: 1.6054e-04\n",
      "Epoch 80/100\n",
      "190/190 [==============================] - 0s 922us/step - loss: 1.6194e-05 - val_loss: 2.6707e-04\n",
      "Epoch 81/100\n",
      "190/190 [==============================] - 0s 1ms/step - loss: 1.6325e-05 - val_loss: 3.0568e-04\n",
      "Epoch 82/100\n",
      "190/190 [==============================] - 0s 921us/step - loss: 1.6217e-05 - val_loss: 1.7887e-04\n",
      "Epoch 83/100\n",
      "190/190 [==============================] - 0s 914us/step - loss: 1.7112e-05 - val_loss: 1.1400e-04\n",
      "Epoch 84/100\n",
      "190/190 [==============================] - 0s 925us/step - loss: 1.6579e-05 - val_loss: 9.8920e-05\n",
      "Epoch 85/100\n",
      "190/190 [==============================] - 0s 928us/step - loss: 1.5922e-05 - val_loss: 1.9094e-04\n",
      "Epoch 86/100\n",
      "190/190 [==============================] - 0s 920us/step - loss: 1.7276e-05 - val_loss: 1.4374e-04\n",
      "Epoch 87/100\n",
      "190/190 [==============================] - 0s 925us/step - loss: 1.9906e-05 - val_loss: 9.0888e-05\n",
      "Epoch 88/100\n",
      "190/190 [==============================] - 0s 961us/step - loss: 1.8171e-05 - val_loss: 8.9590e-05\n",
      "Epoch 89/100\n",
      "190/190 [==============================] - 0s 918us/step - loss: 1.6603e-05 - val_loss: 3.5398e-04\n",
      "Epoch 90/100\n",
      "190/190 [==============================] - 0s 931us/step - loss: 1.7975e-05 - val_loss: 2.5075e-04\n",
      "Epoch 91/100\n",
      "190/190 [==============================] - 0s 953us/step - loss: 1.5454e-05 - val_loss: 2.8731e-04\n",
      "Epoch 92/100\n",
      "190/190 [==============================] - 0s 920us/step - loss: 1.6817e-05 - val_loss: 1.9531e-04\n",
      "Epoch 93/100\n",
      "190/190 [==============================] - 0s 924us/step - loss: 1.8565e-05 - val_loss: 3.0737e-04\n",
      "Epoch 94/100\n",
      "190/190 [==============================] - 0s 919us/step - loss: 1.6764e-05 - val_loss: 1.4538e-04\n",
      "Epoch 95/100\n",
      "190/190 [==============================] - 0s 919us/step - loss: 1.6114e-05 - val_loss: 1.7222e-04\n",
      "Epoch 96/100\n",
      "190/190 [==============================] - 0s 929us/step - loss: 1.6653e-05 - val_loss: 2.6500e-04\n",
      "Epoch 97/100\n",
      "190/190 [==============================] - 0s 928us/step - loss: 1.8242e-05 - val_loss: 3.1428e-04\n",
      "Epoch 98/100\n",
      "190/190 [==============================] - 0s 926us/step - loss: 1.6741e-05 - val_loss: 2.0043e-04\n",
      "Epoch 99/100\n",
      "190/190 [==============================] - 0s 924us/step - loss: 1.6780e-05 - val_loss: 1.5959e-04\n",
      "Epoch 100/100\n",
      "190/190 [==============================] - 0s 932us/step - loss: 1.6826e-05 - val_loss: 2.7351e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x290c14700>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(trainX, trainY, validation_data=(testX, testY), verbose=1, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = model.predict(trainX)\n",
    "testPredict = model.predict(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainPredict[:,0].shape, trainPredict.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate root mean squared error\n",
    "trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))\n",
    "print('Train Score: %.2f RMSE' % (trainScore))\n",
    "\n",
    "testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))\n",
    "print('Test Score: %.2f RMSE' % (testScore))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift train predictions for plotting\n",
    "#we must shift the predictions so that they align on the x-axis with the original dataset. \n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[seq_size:len(trainPredict)+seq_size, :] = trainPredict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(seq_size*2)+1:len(dataset)-1, :] = testPredict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Beispiel-Daten\n",
    "data = {'Date': ['2022-06-18 10:30:00', '2022-06-19 11:45:00', '2022-06-20 12:00:00']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Sicherstellen, dass die 'Date'-Spalte im DateTime-Format ist\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "# # Extrahieren Sie das Datum ohne die Zeitkomponente\n",
    "# df['Date'] = df['Date'].dt.date\n",
    "\n",
    "print(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_time_series(batch_size, n_steps):\n",
    "    freq1, freq2,offsets1, offsets2 = np.random.rand(4,batch_size, 1)\n",
    "    time = np.linspace(0,1, n_steps)\n",
    "    series = 0.5 * np.sin((time - offsets1) * (freq1 * 10 + 10))    # Welle 1\n",
    "    series += 0.2 * np.sin((time - offsets2) * (freq2 * 20 + 20))   # Welle 2\n",
    "    series += 0.1 * (np.random.rand(batch_size, n_steps) - 0.5)     # + Rauschen\n",
    "    return series[..., np.newaxis].astype(np.float32)\n",
    "n_steps = 50\n",
    "\n",
    "series = generate_time_series(10000, n_steps + 1)\n",
    "series.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
